{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing notebook for xInfoDump data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "- `SOURCE_GLOB`: glob expression that will get all the data from your source.\n",
    "- `TEST_ID_REGEX`: Regular Expression with exactly one capture group. This capture group should contain an integer, and will identify the test number across the dataset (in merged CSVs).\n",
    "- `OUTPUT_DIR`: Desired directory to output the dataset to.\n",
    "- `TEST_SCENARIO`: Name of the ns-O-RAN/ns-3 scenario file that was tested (for traceability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_GLOB = os.path.join(\"DataSource\", \"DumpedData*.csv\")\n",
    "TEST_ID_REGEX = \"DumpedData_Test([0-9]+).csv\"\n",
    "OUTPUT_DIR = \"Dataset-0\"\n",
    "TEST_SCENARIO = \"scratch/scenario-zero.cc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OG_FILES = glob.glob(SOURCE_GLOB)\n",
    "print(f\"Loaded {len(OG_FILES)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(OUTPUT_DIR, \"Raw\", \"Separated\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"Split\", \"Separated\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"Processed\", \"Separated\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"Time-Processed\", \"Separated\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENAMING_MAP = {\n",
    "    \"pm-Containers.pLMN-Identity\": \"PLMN ID\",\n",
    "    \"list-of-matched-UEs.ueId\": \"UE ID\",\n",
    "    \"cellObjectID\": \"Cell object ID\",\n",
    "    \"timestamp\": \"Timestamp\",\n",
    "    \"list-of-matched-UEs.pmType\": \"UE Performance Measurement type\",\n",
    "    \"pm-Containers.dl-PRBUsage\": \"DL PRB usage\",\n",
    "    \"pm-Containers.ul-PRBUsage\": \"UL PRB usage\",\n",
    "    \"pm-Containers.dl-TotalofAvailablePRBs\": \"DL total available PRBs\",\n",
    "    \"pm-Containers.ul-TotalofAvailablePRBs\": \"UL total available PRBs\",\n",
    "    \"pm-Containers.nRCGI.nRCellIdentity\": \"NRCI\",\n",
    "    \"list-of-matched-UEs.pmVal\": \"UE Performance Measurement value\",\n",
    "    \"pm-Containers.QCI\": \"QCI\",\n",
    "    \"pm-Containers.drbqci\": \"DRB QCI\",\n",
    "    \"pm-Containers.pDCPBytesDL\": \"DL PCDP Bytes\",\n",
    "    \"pm-Containers.pDCPBytesUL\": \"UL PDCP Bytes\",\n",
    "    \"pm-Containers.pLMN-Identity\": \"PLMN ID\",\n",
    "    \"pm-Containers.interface-type\": \"Interface type\",\n",
    "    \"pm-Containers.numberOfActive-UEs\": \"Number of active UEs\",\n",
    "    \"list-of-matched-UEs.rrcEvent\": \"RRC Event\",\n",
    "    \"list-of-matched-UEs.measResultNeighCells.resultsSSB-Cell.sinr\": \"Neighbor cells SINR\",\n",
    "    \"list-of-matched-UEs.measResultNeighCells.physCellId\": \"Neighbor cell physical cell ID\"\n",
    "}\n",
    "\n",
    "EXTRA_DROP = ['pm-Containers.type',\n",
    " 'pm-Containers.nRCGI.pLMN-Identity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw file copy and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = None\n",
    "for filename in OG_FILES:\n",
    "    test_id = int(re.findall(TEST_ID_REGEX, filename)[0])\n",
    "    shutil.copy2(filename, os.path.join(OUTPUT_DIR, \"Raw\", \"Separated\", f\"DumpedData_Test{test_id}.csv\"))\n",
    "    raw_df = pd.read_csv(filename).rename(columns={\"Unnamed: 0\": \"Original index\"})\n",
    "    raw_df[\"Test ID\"] = test_id\n",
    "    if merged_df is None:\n",
    "        merged_df = raw_df\n",
    "    else:\n",
    "        merged_df = pd.concat([merged_df, raw_df], ignore_index=True)\n",
    "merged_df.to_csv(os.path.join(OUTPUT_DIR, \"Raw\", \"DumpedData_Merged.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process DU data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_du_df = None\n",
    "merged_raw_du_df = None\n",
    "for filename in OG_FILES:\n",
    "    test_id = int(re.findall(TEST_ID_REGEX, filename)[0])\n",
    "    raw_df = pd.read_csv(filename).rename(columns={\"Unnamed: 0\": \"Original index\"})\n",
    "    du_df = raw_df.query(\"`pm-Containers.type` == 'oDU'\").copy().drop(columns=EXTRA_DROP)\n",
    "    du_df = du_df.drop(columns=du_df.columns[du_df.isna().all()].to_list()).reset_index(drop=True)\n",
    "    du_df.to_csv(os.path.join(OUTPUT_DIR, \"Split\", \"Separated\", f\"DUData_Test{test_id}.csv\"))\n",
    "    du_raw_df = du_df.copy()\n",
    "    du_raw_df[\"Test ID\"] = test_id\n",
    "    if merged_raw_du_df is None:\n",
    "        merged_raw_du_df = du_raw_df\n",
    "    else:\n",
    "        merged_raw_du_df = pd.concat([merged_raw_du_df, du_raw_df], ignore_index=True)\n",
    "    du_df = du_df.drop(columns=\"Original index\").rename(columns=RENAMING_MAP).sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "    du_df.to_csv(os.path.join(OUTPUT_DIR, \"Processed\", \"Separated\", f\"DUData_Test{test_id}.csv\"))\n",
    "    du_df[\"Test ID\"] = test_id\n",
    "    if merged_du_df is None:\n",
    "        merged_du_df = du_df\n",
    "    else:\n",
    "        merged_du_df = pd.concat([merged_du_df, du_df], ignore_index=True)\n",
    "    \n",
    "merged_du_df.to_csv(os.path.join(OUTPUT_DIR, \"Processed\", \"DUData_Merged.csv\"))\n",
    "merged_raw_du_df.to_csv(os.path.join(OUTPUT_DIR, \"Split\", \"DUData_Merged.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process CU-UP data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_cu_up_df = None\n",
    "merged_raw_cu_up_df = None\n",
    "for filename in OG_FILES:\n",
    "    test_id = int(re.findall(TEST_ID_REGEX, filename)[0])\n",
    "    raw_df = pd.read_csv(filename).rename(columns={\"Unnamed: 0\": \"Original index\"})\n",
    "    cu_up_df = raw_df.query(\"`pm-Containers.type` == 'oCU-UP'\").copy().drop(columns=EXTRA_DROP)\n",
    "    cu_up_df = cu_up_df.drop(columns=cu_up_df.columns[cu_up_df.isna().all()].to_list()).reset_index(drop=True)\n",
    "    cu_up_df.to_csv(os.path.join(\"Split\", \"Separated\", f\"CU-UPData_Test{test_id}.csv\"))\n",
    "    cu_up_raw_df = cu_up_df.copy()\n",
    "    cu_up_raw_df[\"Test ID\"] = test_id\n",
    "    if merged_raw_cu_up_df is None:\n",
    "        merged_raw_cu_up_df = cu_up_raw_df\n",
    "    else:\n",
    "        merged_raw_cu_up_df = pd.concat([merged_raw_cu_up_df, cu_up_raw_df], ignore_index=True)\n",
    "    cu_up_df = cu_up_df.drop(columns=\"Original index\").rename(columns=RENAMING_MAP).sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "    cu_up_df.to_csv(os.path.join(\"Processed\", \"Separated\", f\"CU-UPData_Test{test_id}.csv\"))\n",
    "    cu_up_df[\"Test ID\"] = test_id\n",
    "    if merged_cu_up_df is None:\n",
    "        merged_cu_up_df = cu_up_df\n",
    "    else:\n",
    "        merged_cu_up_df = pd.concat([merged_cu_up_df, cu_up_df], ignore_index=True)\n",
    "    \n",
    "merged_cu_up_df.to_csv(os.path.join(\"Processed\", \"CU-UPData_Merged.csv\"))\n",
    "merged_raw_cu_up_df.to_csv(os.path.join(\"Split\", \"CU-UPData_Merged.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process CU-CP data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_cu_cp_df = None\n",
    "merged_raw_cu_cp_df = None\n",
    "for filename in OG_FILES:\n",
    "    test_id = int(re.findall(TEST_ID_REGEX, filename)[0])\n",
    "    raw_df = pd.read_csv(filename).rename(columns={\"Unnamed: 0\": \"Original index\"})\n",
    "    cu_cp_df = raw_df.query(\"`pm-Containers.type` == 'oCU-CP'\").copy().drop(columns=EXTRA_DROP)\n",
    "    cu_cp_df = cu_cp_df.drop(columns=cu_cp_df.columns[cu_cp_df.isna().all()].to_list()).reset_index(drop=True)\n",
    "    cu_cp_df.to_csv(os.path.join(\"Split\", \"Separated\", f\"CU-CPData_Test{test_id}.csv\"))\n",
    "    cu_cp_raw_df = cu_cp_df.copy()\n",
    "    cu_cp_raw_df[\"Test ID\"] = test_id\n",
    "    if merged_raw_cu_cp_df is None:\n",
    "        merged_raw_cu_cp_df = cu_cp_raw_df\n",
    "    else:\n",
    "        merged_raw_cu_cp_df = pd.concat([merged_raw_cu_cp_df, cu_cp_raw_df], ignore_index=True)\n",
    "    cu_cp_df = cu_cp_df.drop(columns=\"Original index\").rename(columns=RENAMING_MAP).sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "    cu_cp_df.to_csv(os.path.join(\"Processed\", \"Separated\", f\"CU-CPData_Test{test_id}.csv\"))\n",
    "    cu_cp_df[\"Test ID\"] = test_id\n",
    "    if merged_cu_cp_df is None:\n",
    "        merged_cu_cp_df = cu_cp_df\n",
    "    else:\n",
    "        merged_cu_cp_df = pd.concat([merged_cu_cp_df, cu_cp_df], ignore_index=True)\n",
    "    \n",
    "merged_cu_cp_df.to_csv(os.path.join(\"Processed\", \"CU-CPData_Merged.csv\"))\n",
    "merged_raw_cu_cp_df.to_csv(os.path.join(\"Split\", \"CU-CPData_Merged.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-process all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_SEPARATED_CSVs = glob.glob(os.path.join(OUTPUT_DIR, \"Processed\", \"Separated\", \"*.csv\"))\n",
    "REPROCESS_REGEX = \"Test([0-9]+).csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time_cu_cp_processed_df = None\n",
    "all_time_cu_up_processed_df = None\n",
    "all_time_du_processed_df = None\n",
    "for filename in PROCESSED_SEPARATED_CSVs:\n",
    "    bname = os.path.basename(filename)\n",
    "    type_id = bname.split('_')[0]\n",
    "    test_id = int(re.findall(REPROCESS_REGEX, filename)[0])\n",
    "    data_df = pd.read_csv(filename).drop(columns=[\"Unnamed: 0\"])\n",
    "    first_ts = data_df[\"Timestamp\"].min()\n",
    "    data_df[\"Timestamp\"] = data_df[\"Timestamp\"] - first_ts\n",
    "    data_df.to_csv(os.path.join(OUTPUT_DIR, \"Time-Processed\", \"Separated\", bname))\n",
    "    data_df[\"Test ID\"] = test_id\n",
    "    if type_id == \"CU-CPData\":\n",
    "        if all_time_cu_cp_processed_df is None:\n",
    "            all_time_cu_cp_processed_df = data_df\n",
    "        else:\n",
    "            all_time_cu_cp_processed_df = pd.concat([all_time_cu_cp_processed_df, data_df])\n",
    "    elif type_id == \"CU-UPData\":\n",
    "        if all_time_cu_up_processed_df is None:\n",
    "            all_time_cu_up_processed_df = data_df\n",
    "        else:\n",
    "            all_time_cu_up_processed_df = pd.concat([all_time_cu_up_processed_df, data_df])\n",
    "    elif type_id == \"DUData\":\n",
    "        if all_time_du_processed_df is None:\n",
    "            all_time_du_processed_df = data_df\n",
    "        else:\n",
    "            all_time_du_processed_df = pd.concat([all_time_du_processed_df, data_df])\n",
    "all_time_cu_cp_processed_df.sort_values([\"Test ID\", \"Timestamp\"]).reset_index(drop=True).to_csv(os.path.join(\"Time-Processed\", \"CU-CPData_Merged.csv\"))\n",
    "all_time_cu_up_processed_df.sort_values([\"Test ID\", \"Timestamp\"]).reset_index(drop=True).to_csv(os.path.join(\"Time-Processed\", \"CU-UPData_Merged.csv\"))\n",
    "all_time_du_processed_df.sort_values([\"Test ID\", \"Timestamp\"]).reset_index(drop=True).to_csv(os.path.join(\"Time-Processed\", \"DUData_Merged.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Markdown file generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now(datetime.timezone.utc)\n",
    "pretty_time = now.strftime(\"%A, %-d %B %Y, at %H:%M (%Z)\")\n",
    "iso_time = now.isoformat()\n",
    "template_lines = [\n",
    "    f\"# xInfoDump dataset {OUTPUT_DIR}\\n\",\n",
    "    f\"Exported on {pretty_time}\\n\\n\",\n",
    "    \"## Technical data\\n\",\n",
    "    f\"Test scenario: `{TEST_SCENARIO}`\\n\"\n",
    "    f\"Exportation time: `{iso_time}`\\n\",\n",
    "    f\"Number of tests: {len(OG_FILES)}\\n\\n\",\n",
    "    \"> Automatically generated by the data processing notebook\"\n",
    "]\n",
    "with open(os.path.join(OUTPUT_DIR, \"README.md\"), 'w') as out_md:\n",
    "    out_md.writelines(template_lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
