# urban-endc-simulation
Performance dataset for an urban EN-DC network (3.5 GHz) with user mobility and industrial IoT traffic patterns.
# Urban EN-DC Network Simulation

This repository contains datasets and simulation scenarios representing a 5G EN-DC (E-UTRAN New Radio - Dual Connectivity) network in a dense urban environment. The simulation is designed to evaluate coverage, interference management, and handover performance under realistic traffic conditions.

## ğŸ™ï¸ Scenario Description

The simulated network spans a **4000Ã—4000 meter urban landscape**. It leverages the **EN-DC architecture**, enabling dual connectivity between LTE and mmWave base stations (configured for Sub-6 GHz operation).

The primary goal of this scenario is to mirror real-world network design principles, balancing coverage and interference management in a dense deployment.

![Scenario Topology](path/to/your/image.png)
*Figure 1: Visual representation of the network topology.*

## ğŸš€ Key Features

### 1. Network Topology & Architecture
* **Architecture:** EN-DC enabling dual connectivity between LTE and NR.
    * *Note:* The ns-3 mmWave module is utilized for NR cells, but configured to reflect realistic 5G non-mmWave (Sub-6 GHz) use cases.
* **Deployment:**
    * 1 Central **LTE Macro base station** co-located with an **NR gNB**.
    * 3 additional **NR gNBs** arranged in a circular configuration surrounding the center.
* **Configuration:**
    * Frequency: **3.5 GHz** (Both LTE eNB and NR gNB).
    * Bandwidth: **20 MHz**.
    * Antennas: Phased array antennas with configurable beamforming (initially set to minimal elements for research expansion).
* **Propagation Model:** 3GPP UMi (Urban Microcellular) Street Canyon.

### 2. Mobility & Users
* **User Equipments (UEs):** 12 UEs populated in the environment.
* **Connectivity:** Multi-connectivity enabled (seamless service across LTE and mmWave/NR).
* **Mobility Model:** Random Walk.
    * **Speed:** 2 to 4 m/s.
    * **Traffic Type:** Represents pedestrian and low-speed vehicular traffic.

### 3. Traffic Dynamics
* **Paradigm:** UDP-based traffic generated by a Remote Host connected to the PGW/SGW node.
* **Application Scenario:** High-throughput **Industrial IoT (IIoT)** emulation.
* **Packet Specifications:**
    * Payload: **1280 bytes**.
    * Interval: **500 Âµs**.

### 4. Handover Mechanism
* **Mode:** Dynamic Time-to-Trigger (TTT).
* **SINR Threshold:** 3 dB.
* **Objective:** Enables adaptive cell switching to prevent ping-pong effects while maintaining seamless service stability.

---


## ğŸ”§ Requirements
* ns-3 (Network Simulator 3)
* mmWave module for ns-3
* The dataset in our case was downloaded in CSV format using xSTART framework .
* For further technical details regarding the simulation environment and architecture, please refer to the following publication:

> **xSTART: xApp Simulated Evaluation Environment for Developers**
> *ResearchGate, 2024*
>
> [ğŸ”— Read Full Paper](https://www.researchgate.net/publication/382129862_xSTART_xApp_Simulated_Evaluation_Environment_for_Developers)
>
> # Data Processing Tool

The tool used to process these datasets has been implemented in **Python** (`process_dataset.py`), as well as a **Jupyter Notebook** (`DataProcessing.ipynb`).

## âš™ï¸ Installation

To execute the tool or the notebook, the requirements must be installed first:

```bash
pip3 install -r requirements.txt
```
### ğŸš€ Usage
You can run the processing tool in two ways:

#### Terminal-based UI
Run the Python script directly to use the terminal interface:

```bash
python3 process_dataset.py
```

#### Jupyter Notebook

Open DataProcessing.ipynb in your Jupyter environment.

Set the configuration variables at the start of the notebook.

Execute all cells automatically.

Output
Either method will produce a processed dataset as a result, including:

All sub-processing products.


Detailed documentation of each part of the process (contained within the notebook).

ğŸ“‚ Dataset Directory Structure

Each dataset adheres to the same structure as Dataset-0:

```text
Dataset-0
â”œâ”€â”€ README.md                   # Documentation on the dataset (name, exportation date, tests, ns-O-RAN scenario)
â”œâ”€â”€ Raw                         # Raw, unchanged data, in the exact format xInfoDump provides
â”‚   â”œâ”€â”€ DumpedData_Merged.csv   # Merged data of all tests, includes original index as a separate column
â”‚   â””â”€â”€ Separated
â”‚       â”œâ”€â”€ DumpedData_Test1.csv    # Raw data of test 1
â”‚       â”œâ”€â”€ ...
â”‚       â””â”€â”€ DumpedData_Test10.csv   # Raw data of test 10
â”œâ”€â”€ Split                       # Raw data, but split by type (DU, CU-UP, CU-CP). Columns that are consistently empty for each type are removed
â”‚   â”œâ”€â”€ CU-CPData_Merged.csv    # Merged CU-CP data of all tests
â”‚   â”œâ”€â”€ CU-UPData_Merged.csv    # Merged CU-UP data of all tests
â”‚   â”œâ”€â”€ DUData_Merged.csv       # Merged DU data of all tests
â”‚   â””â”€â”€ Separated
â”‚       â”œâ”€â”€ CU-CPData_Test1.csv # CU-CP data of test 1
â”‚       â”œâ”€â”€ ...
â”‚       â”œâ”€â”€ CU-CPData_Test10.csv # CU-CP data of test 10
â”‚       â”œâ”€â”€ CU-UPData_Test1.csv # CU-UP data of test 1
â”‚       â”œâ”€â”€ ...
â”‚       â”œâ”€â”€ CU-UPData_Test10.csv # CU-UP data of test 10
â”‚       â”œâ”€â”€ DUData_Test1.csv    # DU data of test 1
â”‚       â”œâ”€â”€ ...
â”‚       â””â”€â”€ DUData_Test10.csv   # DU data of test 10
â”œâ”€â”€ Processed                   # Data that has been mostly processed. Different types are split and columns have human-readable names, but timestamps are left raw
â”‚   â”œâ”€â”€ CU-CPData_Merged.csv    # Processed and merged CU-CP data of all tests
â”‚   â”œâ”€â”€ CU-UPData_Merged.csv    # Processed and merged CU-UP data of all tests
â”‚   â”œâ”€â”€ DUData_Merged.csv       # Processed and merged DU data of all tests
â”‚   â””â”€â”€ Separated
â”‚       â”œâ”€â”€ CU-CPData_Test1.csv # Processed and sorted by timestamp CU-CP data of test 1
â”‚       â”œâ”€â”€ ...
â”‚       â”œâ”€â”€ CU-CPData_Test10.csv # Processed and sorted by timestamp CU-CP data of test 10
â”‚       â”œâ”€â”€ CU-UPData_Test1.csv # Processed and sorted by timestamp CU-UP data of test 1
â”‚       â”œâ”€â”€ ...
â”‚       â”œâ”€â”€ CU-UPData_Test10.csv # Processed and sorted by timestamp CU-UP data of test 10
â”‚       â”œâ”€â”€ DUData_Test1.csv    # Processed and sorted by timestamp DU data of test 1
â”‚       â”œâ”€â”€ ...
â”‚       â””â”€â”€ DUData_Test10.csv   # Processed and sorted by timestamp DU data of test 10
â””â”€â”€ Time-Processed              # Processed data where the timestamps have also been changed to start at 0 for each test
    â”œâ”€â”€ CU-CPData_Merged.csv    # Time-processed, merged, and sorted by test ID and timestamp CU-CP data of all tests
    â”œâ”€â”€ CU-UPData_Merged.csv    # Time-processed, merged, and sorted by test ID and timestamp CU-UP data of all tests
    â”œâ”€â”€ DUData_Merged.csv       # Time-processed, merged, and sorted by test ID and timestamp DU data of all tests
    â””â”€â”€ Separated
        â”œâ”€â”€ CU-CPData_Test1.csv # Time-processed and sorted by timestamp CU-CP data of test 1
        â”œâ”€â”€ ...
        â”œâ”€â”€ CU-CPData_Test10.csv # Time-processed and sorted by timestamp CU-CP data of test 10
        â”œâ”€â”€ CU-UPData_Test1.csv # Time-processed and sorted by timestamp CU-UP data of test 1
        â”œâ”€â”€ ...
        â”œâ”€â”€ CU-UPData_Test10.csv # Time-processed and sorted by timestamp CU-UP data of test 10
        â”œâ”€â”€ DUData_Test1.csv    # Time-processed and sorted by timestamp DU data of test 1
        â”œâ”€â”€ ...
        â””â”€â”€ DUData_Test10.csv   # Time-processed and sorted by timestamp DU data of test 10
